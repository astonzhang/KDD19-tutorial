{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Pre-training and Fine-tuning\n",
    "\n",
    "In this notebook, you will understand how to implement the BERT model for pre-training, and to fine-tune a pre-trained BERT model for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "First, let's import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "import gluonnlp as nlp\n",
    "from bert import data\n",
    "\n",
    "# utils.py includes some Blocks defined in the previous transformer notebook\n",
    "from utils import PositionalEncoding, MultiHeadAttention, AddNorm, PositionWiseFFN, EncoderBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Different from the transformer encoder, the BERT encoder has an additional embedding for segment information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![segment embedding](bert-embed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(gluon.nn.Block):\n",
    "    def __init__(self, vocab_size, units, hidden_size,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.segment_embed = gluon.nn.Embedding(2, units)\n",
    "        self.word_embed = gluon.nn.Embedding(vocab_size, units)\n",
    "        self.pos_encoding = PositionalEncoding(units, dropout)\n",
    "        self.blks = gluon.nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add(EncoderBlock(units, hidden_size, num_heads, dropout))\n",
    "\n",
    "    def forward(self, words, segments, mask, *args):\n",
    "        X = self.word_embed(words) + self.segment_embed(segments)\n",
    "        X = self.pos_encoding(X)\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8, 768)\n"
     ]
    }
   ],
   "source": [
    "encoder = BERTEncoder(vocab_size=30000, units=768, hidden_size=3072,\n",
    "                      num_heads=12, num_layers=12, dropout=0.1)\n",
    "encoder.initialize()\n",
    "words = nd.random.randint(low=0, high=30000, shape=(2,8))\n",
    "segments = nd.array([[0,0,0,0,1,1,1,1],[0,0,0,1,1,1,1,1]])\n",
    "encodings = encoder(words, segments, None)\n",
    "print(encodings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Model Decoder\n",
    "\n",
    "Masked language modeling is one of the two pre-training task, where random positions are masked and the model needs to reconstruct the masked words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the masked language model decoder, we first use `gather_nd` to pick the dense vectors representing words at masked position. Then a feed-forward network is applied on them, followed by a fully-connected layer to predict the unnormalized score for all words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDecoder(gluon.nn.Block):\n",
    "    def __init__(self, vocab_size, units, **kwargs):\n",
    "        super(MLMDecoder, self).__init__(**kwargs)\n",
    "        self.decoder = gluon.nn.Sequential()\n",
    "        self.decoder.add(gluon.nn.Dense(units, flatten=False))\n",
    "        self.decoder.add(gluon.nn.GELU())\n",
    "        self.decoder.add(gluon.nn.LayerNorm())\n",
    "        self.decoder.add(gluon.nn.Dense(vocab_size, flatten=False))\n",
    "\n",
    "    def forward(self, X, masked_positions, *args):\n",
    "        X = nd.gather_nd(X, masked_positions)\n",
    "        pred = self.decoder(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30000)\n"
     ]
    }
   ],
   "source": [
    "decoder = MLMDecoder(vocab_size=30000, units=768)\n",
    "decoder.initialize()\n",
    "\n",
    "masked_positions = nd.array([[0,1],[4,8]])\n",
    "mlm_pred = decoder(encodings, masked_positions)\n",
    "print(mlm_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Sentence Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSClassifier(gluon.nn.Block):\n",
    "    def __init__(self, vocab_size, units, **kwargs):\n",
    "        super(NSClassifier, self).__init__(**kwargs)\n",
    "        self.classifier = gluon.nn.Sequential()\n",
    "        self.classifier.add(\n",
    "            gluon.nn.Dense(units=units, flatten=False, activation='tanh'))\n",
    "        self.classifier.add(\n",
    "            gluon.nn.Dense(units=2, flatten=False))\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        X = X[:, 0, :]\n",
    "        pred = self.classifier(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "ns_classifier = NSClassifier(vocab_size=30000, units=768)\n",
    "ns_classifier.initialize()\n",
    "\n",
    "pred = ns_classifier(encodings)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the pre-trained BERT model\n",
    "\n",
    "The list of pre-trained BERT models available\n",
    "in GluonNLP can be found\n",
    "[here](../../model_zoo/bert/index.rst).\n",
    "\n",
    "In this\n",
    "tutorial, the BERT model we will use is BERT\n",
    "BASE trained on an uncased corpus of books and\n",
    "the English Wikipedia dataset in the\n",
    "GluonNLP model zoo.\n",
    "\n",
    "### Get BERT\n",
    "\n",
    "Let's first take\n",
    "a look at the BERT model\n",
    "architecture for sentence pair classification below:\n",
    "<div style=\"width:\n",
    "500px;\">![bert-sentence-pair](bert-sentence-pair.png)</div>\n",
    "where the model takes a pair of\n",
    "sequences and pools the representation of the\n",
    "first token in the sequence.\n",
    "Note that the original BERT model was trained for a\n",
    "masked language model and next-sentence prediction tasks, which includes layers\n",
    "for language model decoding and\n",
    "classification. These layers will not be used\n",
    "for fine-tuning the sentence pair classification.\n",
    "\n",
    "We can load the\n",
    "pre-trained BERT fairly easily\n",
    "using the model API in GluonNLP, which returns the vocabulary\n",
    "along with the\n",
    "model. We include the pooler layer of the pre-trained model by setting\n",
    "`use_pooler` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (layer_norm): BERTLayerNorm(scale=True, center=True, eps=1e-12, axis=-1, in_channels=768)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        )\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      )\n",
      "    )\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ctx = mx.gpu(0) if mx.test_utils.list_gpus() else mx.cpu()\n",
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True, ctx=ctx,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "print(bert_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the model for `SentencePair` classification\n",
    "\n",
    "Now that we have loaded\n",
    "the BERT model, we only need to attach an additional layer for classification.\n",
    "The `BERTClassifier` class uses a BERT base model to encode sentence\n",
    "representation, followed by a `nn.Dense` layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = model.classification.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "bert_classifier.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "bert_classifier.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for BERT\n",
    "\n",
    "For this tutorial, we need to do a bit of preprocessing before feeding our data introduced\n",
    "the BERT model. Here we want to leverage the dataset included in the downloaded archive at the\n",
    "beginning of this tutorial.\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "We use\n",
    "the dev set of the\n",
    "Microsoft Research Paraphrase Corpus dataset. The file is\n",
    "named 'dev.tsv'. Let's take a look at the first few lines of the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Quality\t#1 ID\t#2 ID\t#1 String\t#2 String\n",
      "\n",
      "1\t1355540\t1355592\tHe said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\t\" The foodservice pie business does not fit our long-term growth strategy .\n",
      "\n",
      "0\t2029631\t2029565\tMagnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war .\tHis wife said he was \" 100 percent behind George Bush \" and looked forward to using his years of training in the war .\n",
      "\n",
      "0\t487993\t487952\tThe dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat .\tThe dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .\n",
      "\n",
      "1\t1989515\t1989458\tThe AFL-CIO is waiting until October to decide if it will endorse a candidate .\tThe AFL-CIO announced Wednesday that it will decide in October whether to endorse a candidate before the primaries .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tsv_file = io.open('dev.tsv', encoding='utf-8')\n",
    "for i in range(5):\n",
    "    print(tsv_file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains 5 columns, separated by tabs.\n",
    "The header of\n",
    "the file explains each of these columns, although an explanation for each is included\n",
    "here:\n",
    "0. The label indicating whether the two\n",
    "sentences are semantically equivalent\n",
    "1. The id of the first sentence in this\n",
    "sample\n",
    "2. The id of the second sentence in this sample\n",
    "3. The content of the\n",
    "first sentence\n",
    "4. The content of the second sentence\n",
    "\n",
    "For our task, we are\n",
    "interested in the 0th, 3rd and 4th columns.\n",
    "To load this dataset, we can use the\n",
    "`TSVDataset` API and skip the first line because it's just the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\n",
      "\" The foodservice pie business does not fit our long-term growth strategy .\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Skip the first line, which is the schema\n",
    "num_discard_samples = 1\n",
    "# Split fields by tabs\n",
    "field_separator = nlp.data.Splitter('\\t')\n",
    "# Fields to select from the file\n",
    "field_indices = [3, 4, 0]\n",
    "data_train_raw = nlp.data.TSVDataset(filename='dev.tsv',\n",
    "                                 field_separator=field_separator,\n",
    "                                 num_discard_samples=num_discard_samples,\n",
    "                                 field_indices=field_indices)\n",
    "sample_id = 0\n",
    "# Sentence A\n",
    "print(data_train_raw[sample_id][0])\n",
    "# Sentence B\n",
    "print(data_train_raw[sample_id][1])\n",
    "# 1 means equivalent, 0 means not equivalent\n",
    "print(data_train_raw[sample_id][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the pre-trained BERT model, we need to pre-process the data in the same\n",
    "way it was trained. The following figure shows the input representation in BERT:\n",
    "<div style=\"width: 500px;\">![bert-embed](bert-embed.png)</div>\n",
    "\n",
    "We will use\n",
    "`BERTDatasetTransform` to perform the following transformations:\n",
    "- tokenize\n",
    "the\n",
    "input sequences\n",
    "- insert [CLS] at the beginning\n",
    "- insert [SEP] between sentence\n",
    "A and sentence B, and at the end\n",
    "- generate segment ids to indicate whether\n",
    "a token belongs to the first sequence or the second sequence.\n",
    "- generate valid length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2002  2056  1996  9440  2121  7903  2063 11345  2449  2987  1005\n",
      "  1056  4906  1996  2194  1005  1055  2146  1011  2744  3930  5656  1012\n",
      "     3  1000  1996  9440  2121  7903  2063 11345  2449  2515  2025  4906\n",
      "  2256  2146  1011  2744  3930  5656  1012     3     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "44\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Use the vocabulary from pre-trained model for tokenization\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "\n",
    "# The maximum length of an input sequence\n",
    "max_len = 128\n",
    "\n",
    "# The labels for the two classes [(0 = not similar) or  (1 = similar)]\n",
    "all_labels = [\"0\", \"1\"]\n",
    "\n",
    "# whether to transform the data as sentence pairs.\n",
    "# for single sentence classification, set pair=False\n",
    "# for regression task, set class_labels=None\n",
    "# for inference without label available, set has_label=False\n",
    "pair = True\n",
    "transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                                class_labels=all_labels,\n",
    "                                                has_label=True,\n",
    "                                                pad=True,\n",
    "                                                pair=pair)\n",
    "data_train = data_train_raw.transform(transform)\n",
    "\n",
    "print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "print('label = \\n%s'%data_train[sample_id][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "Now we have all the pieces to put together, and we can finally start fine-tuning the\n",
    "model with very few epochs. For demonstration, we use a fixed learning rate and\n",
    "skip the validation steps. For the optimizer, we leverage the ADAM optimizer which\n",
    "performs very well for NLP data and for BERT models in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4/18] loss=0.7212, lr=0.0000050, acc=0.458\n",
      "[Epoch 0 Batch 8/18] loss=0.7397, lr=0.0000050, acc=0.430\n",
      "[Epoch 0 Batch 12/18] loss=0.7349, lr=0.0000050, acc=0.442\n",
      "[Epoch 0 Batch 16/18] loss=0.7559, lr=0.0000050, acc=0.437\n",
      "[Epoch 1 Batch 4/18] loss=0.8057, lr=0.0000050, acc=0.400\n",
      "[Epoch 1 Batch 8/18] loss=0.7505, lr=0.0000050, acc=0.443\n",
      "[Epoch 1 Batch 12/18] loss=0.6790, lr=0.0000050, acc=0.488\n",
      "[Epoch 1 Batch 16/18] loss=0.6854, lr=0.0000050, acc=0.499\n",
      "[Epoch 2 Batch 4/18] loss=0.6563, lr=0.0000050, acc=0.638\n",
      "[Epoch 2 Batch 8/18] loss=0.5937, lr=0.0000050, acc=0.676\n",
      "[Epoch 2 Batch 12/18] loss=0.5657, lr=0.0000050, acc=0.690\n",
      "[Epoch 2 Batch 16/18] loss=0.6428, lr=0.0000050, acc=0.674\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters\n",
    "batch_size = 32\n",
    "lr = 5e-6\n",
    "\n",
    "# The FixedBucketSampler and the DataLoader for making the mini-batches\n",
    "train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)\n",
    "\n",
    "trainer = mx.gluon.Trainer(bert_classifier.collect_params(), 'adam',\n",
    "                           {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# Collect all differentiable parameters\n",
    "# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
    "# The gradients for these params are clipped later\n",
    "params = [p for p in bert_classifier.collect_params().values() if p.grad_req != 'null']\n",
    "grad_clip = 1\n",
    "\n",
    "# Training the model with only three epochs\n",
    "log_interval = 4\n",
    "num_epochs = 3\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):\n",
    "        with mx.autograd.record():\n",
    "\n",
    "            # Load the data to the GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # Forward computation\n",
    "            out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # And backwards computation\n",
    "        ls.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(1)\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "\n",
    "        # Printing vital information\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "            step_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we showed how to fine-tune a sentence pair\n",
    "classification model with pre-trained BERT parameters. In GluonNLP, this can be\n",
    "done with such few, simple steps. All we did was apply a BERT-style data transformation to\n",
    "pre-process the data, automatically download the pre-trained model, and feed the\n",
    "transformed data into the model, all within 50 lines of code!\n",
    "\n",
    "For demonstration purpose, we skipped the warmup learning rate\n",
    "schedule and validation on the dev dataset used in the original\n",
    "implementation. Please visit the\n",
    "[BERT model zoo webpage](../../model_zoo/bert/index.rst), or the scripts/bert folder\n",
    "in the Github repository for the complete fine-tuning scripts.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Devlin, Jacob, et al. \"Bert:\n",
    "Pre-training of deep\n",
    "bidirectional transformers for language understanding.\"\n",
    "arXiv preprint\n",
    "arXiv:1810.04805 (2018).\n",
    "\n",
    "[2] Dolan, William B., and Chris\n",
    "Brockett.\n",
    "\"Automatically constructing a corpus of sentential paraphrases.\"\n",
    "Proceedings of\n",
    "the Third International Workshop on Paraphrasing (IWP2005). 2005.\n",
    "\n",
    "[3] Peters,\n",
    "Matthew E., et al. \"Deep contextualized word representations.\" arXiv\n",
    "preprint\n",
    "arXiv:1802.05365 (2018)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
